from __future__ import annotations

import logging
logger = logging.getLogger(__name__)

import re
from datetime import date
import datetime as dt
from dataclasses import dataclass, field
from typing import Optional, List, Dict, Any, Tuple

# -----------------------------
# Dates helpers (for fallback)
# -----------------------------

# -----------------------------
# Work-experience fallback helpers
# -----------------------------
_IGNORE_HEADERS_RE = re.compile(
    r"^\s*(–û–°–û–ë–ò–°–¢–Ü\s+–Ø–ö–û–°–¢–Ü|–û–ë–û–í['‚Äô]?\s*–Ø–ó–ö–ò|–û–ë–û–í–Ø–ó–ö–ò|–û–ë–Ø–ó–ê–ù–ù–û–°–¢–ò)\s*:?\s*$",
    re.IGNORECASE
)

_BULLET_PREFIX_RE = re.compile(r"^\s*[‚Ä¢\-\*\u2022]+\s*(?:[‚Ä¢\-\*\u2022]+\s*)*")  # "‚Ä¢", "‚Ä¢ ‚Ä¢", "- -", etc.

# –í–∞—Ä–∏–∞–Ω—Ç—ã –¥–∞—Ç –≤ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ:
# "2020 ‚Äì 2025", "2020-2025", "2020 ‚Äî 2025", "2020 ‚Äì –Ω–∏–Ω—ñ", "2020 ‚Äì now"
_INLINE_YEARS_RE = re.compile(
    r"(?P<start>\d{4})\s*[‚Äì‚Äî\-]\s*(?P<end>\d{4}|–Ω–∏–Ω—ñ|—Ç–µ–ø–µ—Ä|present|now)\b",
    re.IGNORECASE
)

# –õ—é–±–∞—è "–¥–∞—Ç–∞-–ø–æ–¥–æ–±–Ω–∞—è" –ø–æ–¥–ø–∏—Å—å —á—Ç–æ–±—ã –¥–µ—Ç–µ–∫—Ç–∏—Ç—å –Ω–∞—á–∞–ª–æ –Ω–æ–≤–æ–≥–æ –±–ª–æ–∫–∞,
# –µ—Å–ª–∏ —Ç–≤–æ–π –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç
_ANY_DATES_HINT_RE = re.compile(
    r"(\b\d{2}\.\d{4}\b|\b\d{4}\b\s*[‚Äì‚Äî\-]\s*(\b\d{4}\b|–Ω–∏–Ω—ñ|—Ç–µ–ø–µ—Ä|present|now))",
    re.IGNORECASE
)

_MONTHS = {
    # UA
    "—Å—ñ—á": 1, "–ª—é—Ç": 2, "–±–µ—Ä": 3, "–∫–≤—ñ—Ç": 4, "—Ç—Ä–∞–≤": 5, "—á–µ—Ä–≤": 6,
    "–ª–∏–ø": 7, "—Å–µ—Ä–ø": 8, "–≤–µ—Ä": 9, "–∂–æ–≤—Ç": 10, "–ª–∏—Å—Ç": 11, "–≥—Ä—É–¥": 12,
    # RU
    "—è–Ω–≤": 1, "—Ñ–µ–≤": 2, "–º–∞—Ä": 3, "–∞–ø—Ä": 4, "–º–∞–π": 5, "–∏—é–Ω": 6,
    "–∏—é–ª": 7, "–∞–≤–≥": 8, "—Å–µ–Ω": 9, "–æ–∫—Ç": 10, "–Ω–æ—è": 11, "–¥–µ–∫": 12,
}

DEGREE_MAP = [
    ("–Ω–µ–∑–∞–∫—ñ–Ω—á–µ–Ω–∞ –≤–∏—â–∞", ["–Ω–µ–∑–∞–∫—ñ–Ω—á–µ–Ω–∞", "–Ω–µ–∑–∞–∫—ñ–Ω—á–µ–Ω", "–Ω–µ–ø–æ–≤–Ω–∞ –≤–∏—â–∞", "incomplete higher"]),
    ("–≤–∏—â–∞", ["–≤–∏—â–∞", "–≤—ã—Å—à–µ–µ", "higher"]),
    ("—Å–µ—Ä–µ–¥–Ω—è —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–∞", ["—Å–µ—Ä–µ–¥–Ω—è —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–∞", "—Å—Ä–µ–¥–Ω–µ–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ", "vocational", "college"]),
    ("—Å–µ—Ä–µ–¥–Ω—è", ["—Å–µ—Ä–µ–¥–Ω—è", "—Å—Ä–µ–¥–Ω–µ–µ", "secondary"]),
]

_RE_DATE_LINE = re.compile(
    r"(?i)^\s*(?:–∑|—Å)\s*\d{2}\.\d{4}\s*(?:–ø–æ|–¥–æ)\s*(?:\d{2}\.\d{4}|–Ω–∏–Ω—ñ|—Ç–µ–ø–µ—Ä|—Å—å–æ–≥–æ–¥–Ω—ñ|present)\s*(?:\([^)]*\))?\s*$"
)

_RE_MMYYYY = re.compile(r"(\d{2})\.(\d{4})")

OPF_TOKENS = {
    "—Ç–æ–≤", "—Ç–∑–æ–≤", "–æ–æ–æ", "–ø–ø", "–¥–ø", "–ø—Äat", "–ø–∞—Ç", "–∞—Ç", "–ø—Ä–∞—Ç",
    "—Ñ–æ–ø", "—Ñ–ª–ø", "—ñ–ø", "—É–ø—Å–ø", "—Ç–æv"  # –º–æ–∂–Ω–æ –¥–æ–ø–æ–ª–Ω—è—Ç—å
}

HEADER_PREFIX_RE = re.compile(
    r"^\s*(–æ–±–æ–≤[‚Äô'`]?—è–∑–∫–∏|–æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏|–æ–±–æ–≤—è–∑–∫–∏|osobysti yakosti|–æ—Å–æ–±–∏—Å—Ç—ñ —è–∫–æ—Å—Ç—ñ)\s*:?\s*",
    re.IGNORECASE
)

BULLET_RE = re.compile(r"^\s*([‚Ä¢\-\*]+|\d+[\.\)\:])\s+")

DATE_PAIR_LINE_RE = re.compile(
    r"^\s*(–∑|—ñ–∑|from)\s+(.+?)\s+(–ø–æ|to)\s+(.+?)\s*[\-‚Äì‚Äî]\s*(.+)$",
    re.IGNORECASE
)

DUTIES_PREFIX_RE = re.compile(
    r"^\s*(–æ–±–æ–≤[‚Äô']?—è–∑–∫–∏|–æ–±–æ–≤'—è–∑–∫–∏|–æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏|duties)\s*:\s*",
    re.IGNORECASE
)

ROLE_PREFIX_RE = re.compile(r"\s*(?:-|‚Äì|‚Äî|:|\()\s*", re.UNICODE)

# -----------------------------
# Driving categories
# -----------------------------
_DRIVING_RE = re.compile(r"\b–∫–∞—Ç\.?\s*([A-Z–ê-–Ø]{1,2})\b", re.IGNORECASE)

def extract_driving_categories(text: str) -> List[str]:
    cats = set()
    for m in _DRIVING_RE.finditer(text or ""):
        cats.add(m.group(1).upper())
    for c in re.findall(r"\b(A|B|C|D|BE|CE|DE)\b", text or ""):
        cats.add(c.upper())
    order = ["A", "B", "BE", "C", "CE", "D", "DE"]
    return [c for c in order if c in cats]

def driving_cats_from_skill_months(skill_months: Optional[Dict[str, int]]) -> List[str]:
    """
    Extract driving categories from normalized_skill keys of skill_months mapping.
    Works even if your pipeline doesn't build skills_structured.
    """
    cats: List[str] = []
    for k in (skill_months or {}).keys():
        for c in extract_driving_categories(k):
            if c not in cats:
                cats.append(c)
    return cats


# -----------------------------
# Language parsing
# -----------------------------
_LEVEL_ALIASES = {
    "–ø–æ—á–∞—Ç–∫–æ–≤–∏–π": ["–ø–æ—á–∞—Ç–∫–æ–≤–∏–π", "beginner", "elementary", "a1", "a2"],
    "—Å–µ—Ä–µ–¥–Ω—ñ–π": ["—Å–µ—Ä–µ–¥–Ω—ñ–π", "intermediate", "b1"],
    "–≤–∏—â–µ —Å–µ—Ä–µ–¥–Ω—å–æ–≥–æ": ["–≤–∏—â–µ —Å–µ—Ä–µ–¥–Ω—å–æ–≥–æ", "upper intermediate", "upper-intermediate", "b2"],
    "–ø—Ä–æ—Å—É–Ω—É—Ç–∏–π": ["–ø—Ä–æ—Å—É–Ω—É—Ç–∏–π", "advanced", "c1"],
    "–≤—ñ–ª—å–Ω–æ": ["–≤—ñ–ª—å–Ω–æ", "fluent", "native", "c2"],
}

def _norm_text(s: str) -> str:
    s = (s or "").strip().lower()
    s = s.replace("‚Äî", "-").replace("‚Äì", "-")
    s = re.sub(r"\s+", " ", s)
    return s

def _detect_level(s: str) -> Optional[str]:
    t = _norm_text(s)
    for lvl, aliases in _LEVEL_ALIASES.items():
        for a in aliases:
            a2 = _norm_text(a)
            if re.search(rf"(^|[^a-z0-9]){re.escape(a2)}([^a-z0-9]|$)", t):
                return lvl
    return None

def parse_language_item(item: Any) -> Optional[Dict[str, Any]]:
    """
    Parses items like:
      - "–ê–Ω–≥–ª—ñ–π—Å—å–∫–∞ ‚Äî —Å–µ—Ä–µ–¥–Ω—ñ–π"
      - "Deutsch B2"
      - "Polski: –≤—ñ–ª—å–Ω–æ"
    """
    if item is None:
        return None

    s = str(item).strip()
    if not s:
        return None

    parts = re.split(r"[-‚Äì‚Äî:]", s, maxsplit=1)

    if len(parts) == 2:
        lang = parts[0].strip()
        rest = parts[1].strip()
    else:
        tokens = s.split()
        if not tokens:
            return None
        lang = tokens[0]
        rest = s[len(lang):].strip()

    level = _detect_level(rest)
    if not lang:
        return None

    return {"language": lang, "level": level}

def _split_title_into_role_candidates(title: str) -> List[str]:
    """
    –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–ª–æ–∂–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ –ø–æ –¥–µ—Ñ–∏—Å–∞–º, / –∏ —Ç.–ø.
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ä–æ–ª–µ–π-–∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö, –±–µ–∑ –º—É—Å–æ—Ä–∞).
    """
    t = (title or "").strip()
    if not t:
        return []
    # –∫–ª—é—á–µ–≤–æ–π —Å–ø–ª–∏—Ç: –ø–æ '-' (–∏ –¥–ª–∏–Ω–Ω—ã–º —Ç–∏—Ä–µ) + –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø–æ '/'
    parts = re.split(r"\s*(?:-|‚Äì|‚Äî|/)\s*", t)
    parts = [p.strip() for p in parts if p and p.strip()]
    # –∑–∞—â–∏—Ç–∏–º—Å—è –æ—Ç –º—É—Å–æ—Ä–∞: —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ/—Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ
    parts = [p for p in parts if 2 <= len(p) <= 80]
    # –¥–µ–¥—É–ø
    out = []
    seen = set()
    for p in parts:
        k = p.lower()
        if k not in seen:
            seen.add(k)
            out.append(p)
    return out

def _normalize_ws(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def _find_role_prefix_positions(duties_text: str, role: str) -> List[int]:
    """
    –ò—â–µ–º –≤ duties_text –≤—Ö–æ–∂–¥–µ–Ω–∏—è role, –ø–æ—Å–ª–µ –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ä–∞–∑—É –∏–¥—ë—Ç –æ–¥–∏–Ω –∏–∑ –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤: -, :, (
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–∑–∏—Ü–∏–∏ start-–∏–Ω–¥–µ–∫—Å–æ–≤ —ç—Ç–∏—Ö –≤—Ö–æ–∂–¥–µ–Ω–∏–π (–≤ –∏—Å—Ö–æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ).
    """
    if not duties_text or not role:
        return []
    dt = duties_text
    # –∏—â–µ–º role –∫–∞–∫ —Å–ª–æ–≤–æ/—Ñ—Ä–∞–∑—É, –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ whole-word (–Ω–æ —Å –≥—Ä–∞–Ω–∏—Ü–∞–º–∏ –ø–æ –±—É–∫–≤–∞–º –ª—É—á—à–µ)
    pattern = re.compile(rf"(?i)(?<!\w){re.escape(role)}(?!\w)\s*(?:-|‚Äì|‚Äî|:|\()", re.UNICODE)
    return [m.start() for m in pattern.finditer(dt)]

def _split_duties_by_role_prefixes(title: str, duties_text: str) -> Optional[List[Tuple[str, str]]]:
    roles = _split_title_into_role_candidates(title)
    if len(roles) < 2:
        return None

    dt = _normalize_ws(duties_text)
    if not dt:
        return None

    hits = []
    for r in roles:
        pos_list = _find_role_prefix_positions(dt, r)
        if not pos_list:
            return None
        hits.append((pos_list[0], r))  # –±–µ—Ä—ë–º –ø–µ—Ä–≤–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ —Ä–æ–ª–∏

    # —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ–∑–∏—Ü–∏–∏ –≤ —Ç–µ–∫—Å—Ç–µ ‚Äî —ç—Ç–æ –∏ –µ—Å—Ç—å —Ä–µ–∞–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
    hits.sort(key=lambda x: x[0])

    # üî• –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ —Ä–æ–ª–∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã –∏ –≤—Å–µ –∏–∑ title –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –Ω–∞–π–¥–µ–Ω—ã
    roles_lower_set = {r.lower() for r in roles}
    hits_roles_lower = [r.lower() for _, r in hits]

    if len(hits_roles_lower) != len(roles_lower_set):
        return None  # –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–ª–∏ –ø–æ—Ç–µ—Ä—è–Ω–Ω—ã–µ —Ä–æ–ª–∏

    if set(hits_roles_lower) != roles_lower_set:
        return None  # –Ω–∞—à–ª–∏ –Ω–µ —Ç–µ —Ä–æ–ª–∏

    segments: List[Tuple[str, str]] = []
    for idx, (start_pos, role) in enumerate(hits):
        m = re.search(
            rf"(?i)(?<!\w){re.escape(role)}(?!\w)\s*(?:-|‚Äì|‚Äî|:|\()",
            dt[start_pos:]
        )
        if not m:
            return None

        seg_start = start_pos + m.end()
        seg_end = len(dt) if idx + 1 == len(hits) else hits[idx + 1][0]
        seg_text = dt[seg_start:seg_end].strip()

        if not seg_text:
            return None

        segments.append((role.strip(), seg_text))

    return segments

def split_duties_strict_dot_semi(text: str) -> List[str]:
    t = _normalize_ws(text)
    if not t:
        return []
    # –µ—Å–ª–∏ –µ—Å—Ç—å —Ñ—É–Ω–∫—Ü–∏—è split_outside_parens, –ª—É—á—à–µ –µ–π:
    try:
        parts = split_outside_parens(t, ".;")
    except Exception:
        parts = re.split(r"[.;]+", t)
    out = [p.strip(" -‚Äì‚Äî\t,") for p in parts if p.strip(" -‚Äì‚Äî\t,")]
    return out

def has_duties_prefix(s: str) -> bool:
    return bool(DUTIES_PREFIX_RE.match(s or ""))

def normalize_date_token(tok: str) -> Optional[str]:
    t = (tok or "").strip().lower().replace("—Ä.", "").replace("—Ä–æ–∫–∏", "").replace("—Ä", "").strip()
    # –ø–æ–¥–¥–µ—Ä–∂–∫–∞ dd.mm.yyyy / dd.mm.yy
    m = re.search(r"(\d{1,2})\.(\d{1,2})\.(\d{2,4})", t)
    if m:
        dd, mm, yy = m.group(1), m.group(2), m.group(3)
        if len(yy) == 2:
            yy = "19" + yy  # –≥—Ä—É–±–æ; –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å
        return f"{yy.zfill(4)}-{mm.zfill(2)}"
    # –ø–æ–¥–¥–µ—Ä–∂–∫–∞ mm.yyyy
    m = re.search(r"(\d{1,2})\.(\d{4})", t)
    if m:
        mm, yy = m.group(1), m.group(2)
        return f"{yy}-{mm.zfill(2)}"
    # –µ—Å–ª–∏ "–Ω–∏–Ω—ñ/—Ç–µ–ø–µ—Ä" => present
    if "–Ω–∏–Ω—ñ" in t or "—Ç–µ–ø–µ—Ä" in t or "present" in t or "–¥–æ—Ç–µ–ø–µ—Ä" in t:
        return "present"
    return None

def is_bullet_line(s: str) -> bool:
    return bool(BULLET_RE.match(s or ""))

def clean_bullet_prefix(s: str) -> str:
    return BULLET_RE.sub("", (s or "").strip())

def strip_headers(s: str) -> str:
    return HEADER_PREFIX_RE.sub("", (s or "").strip())

def is_ignorable_header_line(line: str) -> bool:
    return bool(_IGNORE_HEADERS_RE.match((line or "").strip()))

def strip_bullets(line: str) -> str:
    return _BULLET_PREFIX_RE.sub("", (line or "").strip()).strip()

def is_bullet_line(line: str) -> bool:
    return bool(_BULLET_PREFIX_RE.match((line or "").strip()))

def parse_inline_title_dates_meta(line: str) -> Optional[Tuple[str, str, str, int, Optional[str], Optional[str]]]:
    """
    Parse: "–ö—É—Ä‚Äô—î—Ä / –≤–æ–¥—ñ–π (2020 ‚Äì 2025) Some Company (Industry)"
    Returns: (title, start_ym, end_norm, months, company, industry)
      end_norm: "YYYY-12" or "present"
    """
    s = (line or "").strip()
    if not s:
        return None

    # title + (dates) required
    m = re.search(r"^(?P<title>.+?)\s*\((?P<dates>[^)]{3,50})\)\s*(?P<rest>.*)$", s)
    if not m:
        return None

    title = m.group("title").strip()
    dates_part = m.group("dates").strip()
    rest = (m.group("rest") or "").strip()

    # parse years range from dates_part
    m2 = _INLINE_YEARS_RE.search(dates_part)
    if not m2:
        return None

    y1 = int(m2.group("start"))
    end_raw = m2.group("end").lower()

    start_ym = f"{y1:04d}-01"

    if end_raw.isdigit():
        y2 = int(end_raw)
        end_norm = f"{y2:04d}-12"
        months = calc_months(start_ym, end_norm)
    else:
        end_norm = "present"
        months = calc_months(start_ym, _now_ym())

    # company (optional) + (industry optional)
    company = None
    industry = None
    if rest:
        # if ends with "(industry)"
        m3 = re.search(r"^(?P<company>.*?)(?:\s*\((?P<industry>[^)]{2,120})\)\s*)?$", rest)
        if m3:
            company = (m3.group("company") or "").strip() or None
            industry = (m3.group("industry") or "").strip() or None
        else:
            company = rest.strip() or None

    return title, start_ym, end_norm, int(months), company, industry

def duties_from_lines(lines: List[str]) -> Tuple[List[str], str]:
    """
    –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —Å—ã—Ä–æ–π —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ –≤ duties list + duties_text
    –ü—Ä–∞–≤–∏–ª–∞:
      - –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ ("–û–°–û–ë–ò–°–¢–Ü –Ø–ö–û–°–¢–Ü", "–û–ë–û–í'–Ø–ó–ö–ò" –∏ —Ç.–ø.)
      - bullet line -> 1 duty —Ü–µ–ª–∏–∫–æ–º
      - non-bullet -> –¥—Ä–æ–±–∏–º –ø–æ , . ;
    """
    acc: List[str] = []
    for ln in lines:
        ln = (ln or "").strip()
        if not ln:
            continue
        if is_ignorable_header_line(ln):
            continue

        if is_bullet_line(ln):
            d = strip_bullets(ln)
            if d:
                acc.append(d)
        else:
            # –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç: —Ä–∞–∑–±–∏–≤–∞–µ–º
            # (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º bullets splitting)
            parts = re.split(r"[;,\.\u2022‚Ä¢]+", ln)
            for p in parts:
                p = p.strip()
                if p and not is_ignorable_header_line(p):
                    acc.append(p)

    # dedup preserving order
    out = []
    seen = set()
    for x in acc:
        k = re.sub(r"\s+", " ", x.lower()).strip()
        if not k or k in seen:
            continue
        seen.add(k)
        out.append(x)

    duties_text = " ".join(out).strip()
    return out, duties_text

#------------------------

def _ym_to_int(ym: str) -> int:
    # "YYYY-MM" -> YYYY*12 + MM
    y, m = ym.split("-")
    return int(y) * 12 + int(m)

def _now_ym() -> str:
    today = date.today()
    return f"{today.year:04d}-{today.month:02d}"

def calc_months(start_ym: str, end_ym: str) -> int:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Å—è—Ü–µ–≤ –º–µ–∂–¥—É start_ym –∏ end_ym –≤–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ.
    –ü—Ä–∏–º–µ—Ä: 2018-03..2018-03 => 1
            2018-03..2019-06 => 16
    """
    if not start_ym or not end_ym:
        return 0
    a = _ym_to_int(start_ym)
    b = _ym_to_int(end_ym)
    if b < a:
        return 0
    return (b - a) + 1

def _month_from_token(tok: str) -> Optional[int]:
    t = tok.strip().lower()
    t3 = t[:3]
    return _MONTHS.get(t3)

def parse_ym(token: str) -> Optional[dt.date]:
    """
    –ü–æ–¥–¥–µ—Ä–∂–∫–∞:
      04.2017
      2017-04
      –∫–≤—ñ—Ç–µ–Ω—å 2017 / –∞–ø—Ä–µ–ª—å 2017 (–±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ 3 –±—É–∫–≤—ã)
    """
    s = token.strip()

    m = re.match(r"^(\d{2})\.(\d{4})$", s)
    if m:
        mm, yy = int(m.group(1)), int(m.group(2))
        return dt.date(yy, mm, 1)

    m = re.match(r"^(\d{4})-(\d{2})$", s)
    if m:
        yy, mm = int(m.group(1)), int(m.group(2))
        return dt.date(yy, mm, 1)

    m = re.match(r"^([A-Za-z–ê-–Ø–∞-—è–Ü—ñ–á—ó–Ñ—î“ê“ë]+)\s+(\d{4})$", s)
    if m:
        mon = _month_from_token(m.group(1))
        yy = int(m.group(2))
        if mon:
            return dt.date(yy, mon, 1)

    return None

def months_between(a: dt.date, b: dt.date) -> int:
    if b < a:
        return 0
    return (b.year - a.year) * 12 + (b.month - a.month) + 1

def fmt_years_1dp(months: int) -> Optional[float]:
    if months <= 0:
        return None
    years = months / 12.0
    return float(f"{years:.1f}")

def is_dates_meta_line(s: str) -> bool:
    """
    –ü—Ä–∏–º–µ—Ä:
    '–∑ 10.2019 –ø–æ 02.2022 (2 —Ä–æ–∫–∏ 5 –º—ñ—Å—è—Ü—ñ–≤) Ostriv, –ö–∏–µ–≤ (–†–æ–∑–Ω–∏—á–Ω–∞—è —Ç–æ—Ä–≥–æ–≤–ª—è)'
    –∏–ª–∏
    '–∑ 01.2004 –ø–æ –Ω–∏–Ω—ñ (22 —Ä–æ–∫–∏) –í–ª–∞—Å–Ω–µ –∞–≤—Ç–æ (–ü—Ä–∏–≤–∞—Ç–Ω—ñ –æ—Å–æ–±–∏)'
    """
    t = (s or "").strip()
    if not t:
        return False
    # –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ "–∑/—Å" –∏ "–ø–æ/–¥–æ"
    if not re.search(r"(?i)\b(–∑|—Å)\b", t):
        return False
    if not re.search(r"(?i)\b(–ø–æ|–¥–æ)\b", t):
        return False
    # —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ –¥–∞—Ç–∞ MM.YYYY
    if not _RE_MMYYYY.search(t):
        return False
    return True

def _to_yyyy_mm(mm: str, yyyy: str) -> str:
    return f"{yyyy}-{mm}"

def looks_like_title(s: str) -> bool:
    t = (s or "").strip()
    if not t:
        return False
    if is_dates_meta_line(t):
        return False
    if len(t) > 80:
        return False
    return True

def looks_like_city(token: str) -> bool:
    t = (token or "").strip()
    if not t:
        return False

    low = t.lower().strip(' "\'‚Äú‚Äù¬´¬ª()')
    if low in OPF_TOKENS:
        return False

    # –µ—Å–ª–∏ –µ—Å—Ç—å –∫–∞–≤—ã—á–∫–∏/–∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã/—Ü–∏—Ñ—Ä—ã ‚Äî —Å–∫–æ—Ä–µ–µ –Ω–µ –≥–æ—Ä–æ–¥
    if re.search(r'[\"‚Äú‚Äù¬´¬ª0-9]', t):
        return False

    # –µ—Å–ª–∏ 3+ —Å–ª–æ–≤ ‚Äî —Å–∫–æ—Ä–µ–µ –Ω–µ –≥–æ—Ä–æ–¥ (—Ç–∏–ø–∞ "–õ–ö –Æ–∫—Ä–µ–π–Ω –ì—Ä—É–ø–ø")
    words = [w for w in re.split(r"\s+", low) if w]
    if len(words) >= 3:
        return False

    # –µ—Å–ª–∏ —ç—Ç–æ "–æ–±–ª." / "—Ä–∞–π–æ–Ω" / "–ª—å–≤—ñ–≤—Å—å–∫–∞ –æ–±–ª." ‚Äî —ç—Ç–æ —Ä–µ–≥–∏–æ–Ω, –º–æ–∂–Ω–æ —Å—á–∏—Ç–∞—Ç—å city/region
    if "–æ–±–ª" in low or "—Ä–∞–π–æ–Ω" in low or "–æ–±–ª–∞—Å—Ç—å" in low:
        return True

    # 1‚Äì2 —Å–ª–æ–≤–∞ ‚Äî –æ–±—ã—á–Ω–æ –≥–æ—Ä–æ–¥
    return len(words) <= 2

def clean_duties_text(s: str) -> str:
    s = (s or "").strip()

    # —É–¥–∞–ª—è–µ–º –≤–µ–¥—É—â–∏–µ –º–∞—Ä–∫–µ—Ä—ã –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–µ–π (—Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã)
    s = re.sub(r"^\s*(–û–ë–û–í['‚Äô]–Ø–ó–ö–ò|–û–ë–û–í–Ø–ó–ö–ò|–û–ë–Ø–ó–ê–ù–ù–û–°–¢–ò|DUTIES)\s*:?\s*", "", s, flags=re.IGNORECASE)

    # –∏–Ω–æ–≥–¥–∞ —Å—Ç—Ä–æ–∫–∞ —Å–æ—Å—Ç–æ–∏—Ç —Ç–æ–ª—å–∫–æ –∏–∑ –º–∞—Ä–∫–µ—Ä–∞
    if re.fullmatch(r"(–û–ë–û–í['‚Äô]–Ø–ó–ö–ò|–û–ë–û–í–Ø–ó–ö–ò|–û–ë–Ø–ó–ê–ù–ù–û–°–¢–ò)\s*:?\s*", s, flags=re.IGNORECASE):
        return ""

    return s.strip()

def split_duties(text: str) -> List[str]:
    raw = (text or "").strip()
    if not raw:
        return []

    # 1) —Ñ–∏–∫—Å–∏—Ä—É–µ–º: –±—ã–ª –ª–∏ –ø—Ä–µ—Ñ–∏–∫—Å "–û–ë–û–í'–Ø–ó–ö–ò:" –≤ –ù–ê–ß–ê–õ–ï –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    prefixed = has_duties_prefix(raw)

    # 2) —á–∏—Å—Ç–∏–º –∑–∞–≥–æ–ª–æ–≤–∫–∏/–º—É—Å–æ—Ä (–Ω–æ prefixed —É–∂–µ –∑–∞–ø–æ–º–Ω–∏–ª–∏)
    s = strip_headers(raw)

    # –µ—Å–ª–∏ —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ ‚Äî –ø—É—Å—Ç–æ
    if not s or s.lower() in {"–æ–±–æ–≤'—è–∑–∫–∏", "–æ–±–æ–≤—è–∑–∫–∏", "–æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏", "–æ—Å–æ–±–∏—Å—Ç—ñ —è–∫–æ—Å—Ç—ñ"}:
        return []

    # 3) –ø–æ—Å—Ç—Ä–æ—á–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ (—á—Ç–æ–±—ã "–û–ë–û–í'–Ø–ó–ö–ò:" –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π –Ω–µ –ª–æ–º–∞–ª–∞ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏)
    lines = [ln.strip() for ln in s.splitlines() if ln.strip()]
    lines = [strip_headers(ln) for ln in lines]
    lines = [ln for ln in lines if ln and ln.lower() not in {"–æ–±–æ–≤'—è–∑–∫–∏", "–æ–±–æ–≤—è–∑–∫–∏", "–æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏", "–æ—Å–æ–±–∏—Å—Ç—ñ —è–∫–æ—Å—Ç—ñ"}]

    if not lines:
        return []

    # 4) –µ—Å–ª–∏ –µ—Å—Ç—å –±—É–ª–ª–µ—Ç—ã ‚Äî –∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ = –æ–¥–Ω–∞ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç—å (–∫–∞–∫ –∏ –±—ã–ª–æ)
    bullet_lines = [ln for ln in lines if is_bullet_line(ln)]
    if bullet_lines:
        out = []
        for ln in bullet_lines:
            ln = clean_bullet_prefix(ln)
            ln = strip_headers(ln)
            if ln:
                out.append(ln)
        return out

    # 5) —Å–æ–±–∏—Ä–∞–µ–º –≤ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç
    s1 = " ".join(lines).strip()
    if not s1:
        return []

    # 6) –ö–õ–Æ–ß–ï–í–û–ï: –µ—Å–ª–∏ –±—ã–ª –ø—Ä–µ—Ñ–∏–∫—Å "–û–ë–û–í'–Ø–ó–ö–ò:" ‚Äî –ù–ï —Ä–µ–∂–µ–º –ø–æ –∑–∞–ø—è—Ç—ã–º –≤–æ–æ–±—â–µ
    if prefixed:
        parts = split_outside_parens(s1, ".;")
        return [p.strip(" -‚Äì‚Äî\t") for p in parts if p.strip(" -‚Äì‚Äî\t")]

    # 7) –∏–Ω–∞—á–µ ‚Äî —Ç–≤–æ—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ (–æ–ø–∏—Å–∞–Ω–∏–µ vs –ø–µ—Ä–µ—á–µ–Ω—å)
    words = len(re.findall(r"\w+", s1))
    commas = s1.count(",")
    if words >= 10 and commas <= max(1, words // 12):
        parts = split_outside_parens(s1, ".;")
        return [p.strip(" -‚Äì‚Äî\t") for p in parts if p.strip(" -‚Äì‚Äî\t")]

    parts = split_outside_parens(s1, ",.;")
    return [p.strip(" -‚Äì‚Äî\t") for p in parts if p.strip(" -‚Äì‚Äî\t")]

def split_tail_parentheses(meta: str):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
      base_text (–±–µ–∑ —Ö–≤–æ—Å—Ç–æ–≤—ã—Ö —Å–∫–æ–±–æ–∫),
      parens: —Å–ø–∏—Å–æ–∫ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å–∫–æ–±–æ–∫ –≤ –∫–æ–Ω—Ü–µ —Å—Ç—Ä–æ–∫–∏ (–ø–æ –ø–æ—Ä—è–¥–∫—É)
    """
    s = (meta or "").strip()
    parens = []

    # —Å–Ω–∏–º–∞–µ–º —Ü–µ–ø–æ—á–∫—É " (...) (...) (...) " —Å –∫–æ–Ω—Ü–∞
    while True:
        m = re.search(r"\s*\(([^()]*)\)\s*$", s)
        if not m:
            break
        parens.insert(0, m.group(1).strip())
        s = s[:m.start()].rstrip()

    return s, parens

def split_outside_parens(text: str, seps: str):
    out = []
    buf = []
    depth = 0
    for ch in text:
        if ch == "(":
            depth += 1
        elif ch == ")":
            depth = max(0, depth - 1)

        if depth == 0 and ch in seps:
            piece = "".join(buf).strip()
            if piece:
                out.append(piece)
            buf = []
        else:
            buf.append(ch)

    tail = "".join(buf).strip()
    if tail:
        out.append(tail)
    return out

def parse_one_line_date_entries(lines: List[str]) -> List["WorkItem"]:
    items = []
    for ln in lines:
        m = DATE_PAIR_LINE_RE.match(ln)
        if not m:
            continue

        start_raw = m.group(2)
        end_raw = m.group(4)
        rest = m.group(5).strip()

        start = normalize_date_token(start_raw)
        end = normalize_date_token(end_raw)

        # rest: "title, title2 ... COMPANY"
        # split –ø–æ –∑–∞–ø—è—Ç—ã–º –≤ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã —Ä–æ–ª–µ–π
        candidates = [x.strip() for x in rest.split(",") if x.strip()]

        # —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –ø–æ—Å–ª–µ–¥–Ω—è—è —á–∞—Å—Ç—å –æ–±—ã—á–Ω–æ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–æ–º–ø–∞–Ω–∏—é (—Ç–∞–º –∫–∞–≤—ã—á–∫–∏, –¢–û–í/–î–ü/–û–û–û –∏ —Ç.–ø.)
        company = None
        title = rest

        if len(candidates) >= 2:
            last = candidates[-1]
            low = last.lower()
            if any(tok in low for tok in OPF_TOKENS) or re.search(r"[¬´¬ª\"‚Äú‚Äù]", last):
                company = last
                title = ", ".join(candidates[:-1])

        # –µ—Å–ª–∏ company –≤—Å–µ –µ—â—ë None ‚Äî –ø–æ–ø—Ä–æ–±—É–π –≤—ã—Ç–∞—â–∏—Ç—å ‚Äú–¢–û–í ‚Ä¶‚Äù –∏–∑ –∫–æ–Ω—Ü–∞ —Å—Ç—Ä–æ–∫–∏
        if company is None:
            m2 = re.search(r"((?:–¢–û–í|–û–û–û|–î–ü|–ü–ü|–ê–¢|–ü—Ä–ê–¢|–§–û–ü)\s+.+)$", rest, re.IGNORECASE)
            if m2:
                company = m2.group(1).strip()
                title = rest[:m2.start()].strip(" -‚Äì‚Äî,")
        
        items.append(
            WorkItem(
                title=title,
                company=company,
                city=None,
                industry=None,
                start=start,
                end=end,
                months=0,  # –º–µ—Å—è—Ü—ã –º–æ–∂–Ω–æ –ø–æ—Å—á–∏—Ç–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ, –µ—Å–ª–∏ —Ö–æ—á–µ—à—å
                duties=[],
                duties_text="",
                block_text=ln.strip(),
            )
        )
    return items

def parse_dates_meta_line(line: str) -> Tuple[str, str, int, str, str, str, str]:
    """
    Returns:
      start, end, months, company, city, industry, duties_hint_text
    duties_hint_text ‚Äî —Ö–≤–æ—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ —É–¥–∞–ª–æ—Å—å —É–≤–µ—Ä–µ–Ω–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ company/city/industry (–º–æ–∂–µ—Ç –ø—Ä–∏–≥–æ–¥–∏—Ç—å—Å—è).
    """
    t = (line or "").strip()

    # 1) start/end
    dates = list(_RE_MMYYYY.finditer(t))
    start = end = ""
    if dates:
        start = _to_yyyy_mm(dates[0].group(1), dates[0].group(2))
    if len(dates) >= 2:
        end = _to_yyyy_mm(dates[1].group(1), dates[1].group(2))
    else:
        if re.search(r"(?i)\b(–Ω–∏–Ω—ñ|—Ç–µ–ø–µ—Ä|—Å—å–æ–≥–æ–¥–Ω—ñ|present)\b", t):
            end = "present"

    end_norm = end  # —Ç–æ —á—Ç–æ —Ç—ã —Ä–∞—Å–ø–∞—Ä—Å–∏–ª

    # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è "–ø–æ –Ω–∏–Ω—ñ"/"present"
    if end_norm in {"present", "now"}:
        end_ym = _now_ym()
    else:
        # –µ—Å–ª–∏ —É —Ç–µ–±—è end_norm —É–∂–µ "YYYY-MM" ‚Äî –æ—Å—Ç–∞–≤—å
        end_ym = end_norm

    months = calc_months(start, end_ym)

    # 2) –æ—Ç—Ä–µ–∑–∞–µ–º –ª–µ–≤—É—é —á–∞—Å—Ç—å –¥–æ –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–π —Å–∫–æ–±–∫–∏ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
    #    –æ–±—ã—á–Ω–æ: "... (2 —Ä–æ–∫–∏ 5 –º—ñ—Å—è—Ü—ñ–≤) <meta...>"
    meta_part = t
    m = re.search(r"\([^)]*\)\s*(.*)$", t)
    if m:
        meta_part = (m.group(1) or "").strip()
    else:
        # –µ—Å–ª–∏ —Å–∫–æ–±–æ–∫ –Ω–µ—Ç ‚Äî –ø—Ä–æ–±—É–µ–º –æ—Ç—Ä–µ–∑–∞—Ç—å –ø–æ—Å–ª–µ "–ø–æ <...>"
        m2 = re.search(r"(?i)\b(–ø–æ|–¥–æ)\b\s*(?:\d{2}\.\d{4}|–Ω–∏–Ω—ñ|—Ç–µ–ø–µ—Ä|—Å—å–æ–≥–æ–¥–Ω—ñ|present)\s*(.*)$", t)
        if m2:
            meta_part = (m2.group(2) or "").strip()

    meta_base, parens = split_tail_parentheses(meta_part)

    company = city = industry = ""
    duties_hint = ""

    # industry = –ø–æ—Å–ª–µ–¥–Ω—è—è —Å–∫–æ–±–∫–∞
    industry = parens[-1].strip() if parens else None

    # city/region = –ø–µ—Ä–≤–∞—è —Å–∫–æ–±–∫–∞, –µ—Å–ª–∏ —Å–∫–æ–±–æ–∫ >= 2
    region = parens[0].strip() if len(parens) >= 2 else None

    # —Ç–µ–ø–µ—Ä—å —Ä–∞–∑–±–æ—Ä meta_base –∫–∞–∫ "company, city" –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ "company"
    parts = [p.strip() for p in meta_base.split(",") if p.strip()]

    company = None
    city = None

    if not parts:
        company = None
        city = None
    else:
        # –ø—ã—Ç–∞–µ–º—Å—è –≤—ã–¥–µ–ª–∏—Ç—å city –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç, –Ω–æ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ city
        if len(parts) >= 2 and looks_like_city(parts[-1]):
            city = parts[-1]
            company = ", ".join(parts[:-1]).strip()
        else:
            company = ", ".join(parts).strip()

    # –µ—Å–ª–∏ region –Ω–∞–π–¥–µ–Ω ‚Äî —ç—Ç–æ —Å–∏–ª—å–Ω–µ–µ, —á–µ–º city –∏–∑ –∑–∞–ø—è—Ç–æ–π
    if region:
        city = region

    return start, end_norm, months, company, city, industry, duties_hint

@dataclass
class WorkItem:
    title: str = ""
    company: str = ""
    city: str = ""
    industry: str = ""      # <-- –Ω–æ–≤–æ–µ
    start: Optional[str] = None    # "YYYY-MM"
    end: Optional[str] = None      # "YYYY-MM" or "present"
    months: Optional[int] = None
    duties: List[str] = field(default_factory=list)   # <-- –Ω–æ–≤–æ–µ
    duties_text: str = ""          # –Ω–µ —Ä–µ–∂–µ–º

    block_text: str = ""  # –≤–µ—Å—å —Ç–µ–∫—Å—Ç –±–ª–æ–∫–∞ –æ–¥–Ω–æ–π –¥–æ–ª–∂–Ω–æ—Å—Ç–∏

@dataclass
class EduItem:
    place: str = ""
    degree: str = ""               # "–±–∞–∫–∞–ª–∞–≤—Ä/–º–∞–≥—ñ—Å—Ç—Ä/..." –µ—Å–ª–∏ –Ω–∞–π–¥—ë–º
    specialty: str = ""
    start: Optional[str] = None
    end: Optional[str] = None
    months: Optional[int] = None
    extra: str = ""

def parse_work_experience_section(text: str) -> List["WorkItem"]:
    lines = [ln.strip() for ln in (text or "").splitlines()]
    lines = [ln for ln in lines if ln]

    items: List[WorkItem] = []

    # -----------------------------
    # 1) Primary parser: your current "title line" + "dates/meta line"
    # -----------------------------
    i = 0
    while i < len(lines):
        if not looks_like_title(lines[i]):
            i += 1
            continue

        title = lines[i].strip()

        if i + 1 >= len(lines) or not is_dates_meta_line(lines[i + 1]):
            i += 1
            continue

        line2 = lines[i + 1]
        start, end, months, company, city, industry, _ = parse_dates_meta_line(line2)

        j = i + 2
        duties_acc = []
        while j < len(lines):
            cur = lines[j].strip()
            nxt = lines[j + 1].strip() if j + 1 < len(lines) else ""

            # –Ω–æ–≤—ã–π –±–ª–æ–∫: title + datesmeta
            if looks_like_title(cur) and nxt and is_dates_meta_line(nxt):
                break

            # –µ—Å–ª–∏ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ –≤—Å—Ç—Ä–µ—Ç–∏–ª–∏ datesmeta –±–µ–∑ title ‚Äî —Å—Ç–æ–ø
            if is_dates_meta_line(cur):
                break

            # –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–µ–π/–∫–∞—á–µ—Å—Ç–≤
            if is_ignorable_header_line(cur):
                j += 1
                continue

            duties_acc.append(cur)
            j += 1

        duties_text = " ".join(duties_acc).strip()

        # 1) –ü–´–¢–ê–ï–ú–°–Ø –†–ê–ó–ë–ò–¢–¨ –û–î–ò–ù –ë–õ–û–ö –ù–ê –ù–ï–°–ö–û–õ–¨–ö–û –†–û–õ–ï–ô
        role_segments = _split_duties_by_role_prefixes(title, duties_text)

        if role_segments:
            # —Å–æ–∑–¥–∞—ë–º –Ω–µ—Å–∫–æ–ª—å–∫–æ WorkItem —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ –¥–∞—Ç–∞–º–∏/–∫–æ–º–ø–∞–Ω–∏–µ–π/–≥–æ—Ä–æ–¥–æ–º/–∏–Ω–¥—É—Å—Ç—Ä–∏–µ–π
            for role_title, role_text in role_segments:
                print(role_text)
                role_duties = split_duties_strict_dot_semi(role_text)  # —Ç–æ–ª—å–∫–æ . –∏ ;
                items.append(
                    WorkItem(
                        title=role_title,
                        company=company,
                        city=city,
                        industry=industry,
                        start=start,
                        end=end,
                        months=int(months) if isinstance(months, int) else 0,
                        duties=role_duties,
                        duties_text=role_text,      # –≤–∞–∂–Ω–æ: –∏–º–µ–Ω–Ω–æ —Ä–æ–ª—å-–∫—É—Å–æ–∫
                        block_text=role_text,       # –µ—Å–ª–∏ —Ç—ã –¥–æ–±–∞–≤–∏–ª –ø–æ–ª–µ "–≤–µ—Å—å —Ç–µ–∫—Å—Ç –±–ª–æ–∫–∞" ‚Äî —Å—é–¥–∞ –∂–µ
                    )
                )
        else:
            # 2) –û–°–¢–ê–õ–¨–ù–û–ï ‚Äî –ö–ê–ö –†–ê–ù–¨–®–ï
            duties = split_duties(duties_text)
            #duties, duties_text = duties_from_lines(duties_acc)

            block_lines = [title, line2] + duties_acc
            block_text = "\n".join(block_lines).strip()

            items.append(
                WorkItem(
                    title=title,
                    company=company,
                    city=city,
                    industry=industry,
                    start=start,
                    end=end,
                    months=int(months) if isinstance(months, int) else 0,
                    duties=duties,
                    duties_text=duties_text,
                    block_text=block_text,
                )
            )
        i = j

    # -----------------------------
    # 2) Fallback parser: "Title (2020‚Äì2025) Company (Industry)" format
    #    Only if primary found nothing
    # -----------------------------
    if not items:
        items2: List[WorkItem] = []
        k = 0
        while k < len(lines):
            ln = lines[k].strip()
            if not ln or is_ignorable_header_line(ln):
                k += 1
                continue

            parsed = parse_inline_title_dates_meta(ln)
            if not parsed:
                k += 1
                continue

            title, start, end, months, company, industry = parsed

            # collect duties until a new inline block or until a "date hint" line that likely starts a new block
            dlines = []
            k += 1
            while k < len(lines):
                cur = lines[k].strip()
                if not cur:
                    k += 1
                    continue

                # stop on ignorable header but do not include
                if is_ignorable_header_line(cur):
                    k += 1
                    continue

                # if new inline pattern starts -> new block
                if parse_inline_title_dates_meta(cur):
                    break

                # if we meet "dates-like" line, treat as new block marker as you requested
                # (this is a heuristic; helps on mixed formats)
                if _ANY_DATES_HINT_RE.search(cur) and looks_like_title(cur):
                    break

                dlines.append(cur)
                k += 1

            duties, duties_text = duties_from_lines(dlines)
            block_lines = [title, line2] + duties_acc
            block_text = "\n".join(block_lines).strip()
            
            items2.append(
                WorkItem(
                    title=title,
                    company=company,
                    city=None,
                    industry=industry,
                    start=start,
                    end=end,
                    months=int(months) if isinstance(months, int) else 0,
                    duties=duties,
                    duties_text=duties_text,
                    block_text=block_text,
                )
            )
        if not items2:
            items2 = parse_one_line_date_entries(lines)
        items = items2
    
    if not items:
        logger.debug("parse_work_experience_section: non-empty text but 0 items (len=%s)", len(text))

    return items

def parse_education_section(text: str, now: Optional[dt.date] = None) -> List[EduItem]:
    if not text:
        return []
    now = now or dt.date.today()

    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    items: List[EduItem] = []
    cur: Optional[EduItem] = None

    # –¥–∏–∞–ø–∞–∑–æ–Ω –ª–µ—Ç: "–∑ 1983 –ø–æ 1990"
    yr_re = re.compile(r"(?:–∑|—Å)\s+(\d{4})\s+(?:–ø–æ|–¥–æ)\s+(\d{4})", re.IGNORECASE)

    def flush():
        nonlocal cur
        if cur:
            items.append(cur)
            cur = None

    for ln in lines:
        m = yr_re.search(ln)
        if m:
            if not cur:
                cur = EduItem()
            y1, y2 = int(m.group(1)), int(m.group(2))
            start_dt = dt.date(y1, 1, 1)
            end_dt = dt.date(y2, 12, 1)
            cur.start = f"{y1:04d}-01"
            cur.end = f"{y2:04d}-12"
            cur.months = months_between(start_dt, end_dt)
            cur.degree = detect_degree(ln)
            cur.place, cur.specialty = parse_edu_place_specialty(ln)
            tail = ln[m.end():].strip(" ,‚Äî-")
            if tail:
                cur.extra = (cur.extra + " " + tail).strip()
            continue

        # —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –µ—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ –ø–æ—Ö–æ–∂–∞ –Ω–∞ ‚Äú—É—á—Ä–µ–∂–¥–µ–Ω–∏–µ‚Äù ‚Äî –Ω–∞—á–Ω—ë–º –Ω–æ–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç
        if len(ln) > 6 and (("—É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç" in ln.lower()) or ("—ñ–Ω—Å—Ç–∏—Ç—É—Ç" in ln.lower()) or ("university" in ln.lower()) or ("academy" in ln.lower())):
            flush()
            cur = EduItem(place=ln)
            continue

        if cur:
            # specialty/degree –æ–±—ã—á–Ω–æ —Ç—É—Ç
            # –Ω–µ —Ä–µ–∂–µ–º ‚Äî —Å–æ—Ö—Ä–∞–Ω–∏–º –≤ specialty/extra
            if not cur.specialty:
                cur.specialty = ln
            else:
                cur.extra = (cur.extra + " " + ln).strip()

    flush()
    return items

def detect_degree(text: str) -> str:
    t = (text or "").lower()
    for norm, keys in DEGREE_MAP:
        for k in keys:
            if k in t:
                return norm
    return ""

def parse_edu_place_specialty(line: str) -> tuple[str, str]:
    """
    –û—á–µ–Ω—å –ø—Ä–æ—Å—Ç–æ–π, –Ω–æ —Ä–∞–±–æ—Ç–∞—é—â–∏–π –¥–ª—è —Ç–≤–æ–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤:
    - place: –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ/–∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞ –¥–æ –ø—Ä–æ–±–µ–ª–∞ (–∏–ª–∏ –¥–æ –ø–µ—Ä–≤–æ–π –∑–∞–ø—è—Ç–æ–π)
    - specialty: –ø–æ—Å–ª–µ —Ç–æ—á–∫–∏ –¥–æ –∑–∞–ø—è—Ç–æ–π (–µ—Å–ª–∏ –µ—Å—Ç—å)
    """
    s = (line or "").strip()
    if not s:
        return "", ""

    # –æ—Ç—Ä–µ–∂–µ–º –¥–∞—Ç—É –∏ —Å–∫–æ–±–∫–∏
    s = re.sub(r"\(\s*\d+.*?\)", "", s).strip()

    # place: –¥–æ –ø–µ—Ä–≤–æ–π –∑–∞–ø—è—Ç–æ–π –∏–ª–∏ –¥–æ "–ö–∏–µ–≤/–ö–∏—ó–≤" –∏ —Ç.–ø.
    head = s.split(",")[0].strip()

    # place —á–∞—Å—Ç–æ –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ (–∫–≥–∏—Ñ–∫/–∫–Ω—Ç—É/–Ω—Ç—É—É...)
    place = head.split()[0].strip() if head else ""

    # specialty: –∫—É—Å–æ–∫ –ø–æ—Å–ª–µ —Ç–æ—á–∫–∏ –¥–æ –∑–∞–ø—è—Ç–æ–π
    specialty = ""
    if "." in head:
        after_dot = head.split(".", 1)[1].strip()
        specialty = after_dot

    return place, specialty